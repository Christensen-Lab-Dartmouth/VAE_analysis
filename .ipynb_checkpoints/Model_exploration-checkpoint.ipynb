{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports and set up the working environment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "# Keras uses TensforFlow backend as default\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Conv1D,UpSampling1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input image dimensions\n",
    "steps = 1\n",
    "original_dim = 28*28 # Take care here since we are changing this according to the data\n",
    "\n",
    "# Number of convolutional filters to use\n",
    "filters = 64\n",
    "\n",
    "# Convolution kernel size\n",
    "num_conv = 6\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Decoder output dimensionality\n",
    "decOutput = 10\n",
    "\n",
    "latent_dim = 20\n",
    "intermediate_dim = 256\n",
    "epsilon_std = 1.0\n",
    "epochs = 5\n",
    "\n",
    "x = Input(batch_shape=(batch_size,steps,original_dim))\n",
    "\n",
    "# Play around with padding here, not sure what to go with.\n",
    "conv_1 = Conv1D(1,\n",
    "                kernel_size=num_conv,\n",
    "                padding='same', \n",
    "                activation='relu')(x)\n",
    "conv_2 = Conv1D(filters,\n",
    "                kernel_size=num_conv,\n",
    "                padding='same', \n",
    "                activation='relu',\n",
    "                strides=1)(conv_1)\n",
    "flat = Flatten()(conv_2) # Since we are passing flat data anyway, we probably don't need this.\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "z_mean = Dense(latent_dim)(hidden)\n",
    "z_log_var = Dense(latent_dim)(hidden)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var ) * epsilon # the original VAE divides z_log_var with two -- why?\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) # Double check wtf this is supposed to be\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean)\n",
    "vae.compile(optimizer='adam', loss=vae_loss) # 'rmsprop'\n",
    "vae.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
